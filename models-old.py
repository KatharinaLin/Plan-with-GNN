# Copyright 2018 The GraphNets Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""Model architectures for the demos."""



import modules
import utils_tf
import blocks
import numpy as np
import tensorflow as tf
import sonnet as snt

NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.
LATENT_SIZE = 16  # Hard-code latent layer sizes for demos.


def make_mlp_model():
  """Instantiates a new MLP, followed by LayerNorm.

  The parameters of each new MLP are not shared with others generated by
  this function.

  Returns:
    A Sonnet module which contains the MLP and LayerNorm.
  """
  regularizers = {"w": tf.contrib.layers.l1_regularizer(scale=0.5),
                  "b": tf.contrib.layers.l2_regularizer(scale=0.5)}
  return snt.Sequential([
      snt.nets.MLP([100,100,100,100], activate_final=True),
      snt.LayerNorm()
  ])

def make_atom_state_mlp_model():
  regularizers = {"w": tf.contrib.layers.l1_regularizer(scale=0.5),
                  "b": tf.contrib.layers.l2_regularizer(scale=0.5)}
  return snt.Sequential([
      snt.nets.MLP([100,100,100,100], activate_final=True),
    snt.LayerNorm()
  ])

def make_heu_mlp_model():
  """Instantiates a new MLP, followed by LayerNorm.

  The parameters of each new MLP are not shared with others generated by
  this function.

  Returns:
    A Sonnet module which contains the MLP and LayerNorm.
  """
  regularizers = {"w": tf.contrib.layers.l1_regularizer(scale=0.5),
                  "b": tf.contrib.layers.l2_regularizer(scale=0.5)}
  initializers = {
        "w": tf.truncated_normal_initializer(stddev=1.0),
        "b": tf.truncated_normal_initializer(stddev=1.0),
  }
  return snt.Sequential([
      snt.nets.MLP([16, 16], activate_final=True, initializers=initializers),
    snt.LayerNorm()
  ])


class MLPGraphIndependent(snt.AbstractModule):
  """GraphIndependent with MLP edge, node, and global models."""

  def __init__(self, name="MLPGraphIndependent"):
    super(MLPGraphIndependent, self).__init__(name=name)
    with self._enter_variable_scope():
      print("over")
      self._network = modules.GraphIndependent(
          edge_model_fn=make_mlp_model,
          node_model_fn=make_mlp_model,
          global_model_fn=make_mlp_model)

  def _build(self, inputs):
    return self._network(inputs)




class Encoder(snt.AbstractModule):
  """GraphIndependent with MLP edge, node models."""

  def __init__(self, name="Encoder"):
    super(Encoder, self).__init__(name=name)
    with self._enter_variable_scope():
      self._network = modules.GraphIndependent(
          edge_model_fn=make_mlp_model)

  def _build(self, inputs):
    return self._network(inputs)



class MLPGraphNetwork(snt.AbstractModule):
  """GraphNetwork with MLP edge, node, and global models."""

  def __init__(self, name="MLPGraphNetwork"):
    super(MLPGraphNetwork, self).__init__(name=name)
    with self._enter_variable_scope():
      self._network = modules.GraphNetwork(make_atom_state_mlp_model, make_mlp_model, make_mlp_model)

  def _build(self, inputs):
    return self._network(inputs)

  def getInitial(self, inputs):
    return self._network.getInitialState(inputs)





class SigmoidEdgeProcessingModule(snt.AbstractModule):
  def __init__(self, edge_output_size, name="sigmoid_edge_output_func"):
    super(SigmoidEdgeProcessingModule, self).__init__(name=name)
    self._edge_output_size = edge_output_size
    with self._enter_variable_scope():  # This line is crucial!
      regularizers = {"w": tf.contrib.layers.l1_regularizer(scale=0.1),
                      "b": tf.contrib.layers.l2_regularizer(scale=0.1)}
      initializers = {"w": tf.truncated_normal_initializer(stddev=1.0),
                      "b": tf.truncated_normal_initializer(stddev=1.0)}
      self._lin_mod = snt.Linear(self._edge_output_size, name="state_edge_output")  # Construct submodule here.

  def _build(self, inputs):
      return tf.nn.sigmoid(self._lin_mod(inputs))  # Connect previously constructed mod.

class TanhEdgeProcessingModule(snt.AbstractModule):
  def __init__(self, edge_output_size, name="tanh_edge_output_func"):
    super(TanhEdgeProcessingModule, self).__init__(name=name)
    self._edge_output_size = edge_output_size
    with self._enter_variable_scope():  # This line is crucial!
      regularizers = {"w": tf.contrib.layers.l1_regularizer(scale=0.1),
                        "b": tf.contrib.layers.l2_regularizer(scale=0.1)}
      initializers = {"w": tf.truncated_normal_initializer(stddev=1.0),
                        "b": tf.truncated_normal_initializer(stddev=1.0)}
      self._lin_mod = snt.Linear(self._edge_output_size, name="other_edge_output")  # Construct submodule here.

  def _build(self, inputs):
    return tf.nn.tanh(self._lin_mod(inputs))  # Connect previously constructed mod.

class EncodeProcessDecode(snt.AbstractModule):
  """Full encode-process-decode model.

  The model we explore includes three components:
  - An "Encoder" graph net, which independently encodes the edge, node, and
    global attributes (does not compute relations etc.).
  - A "Core" graph net, which performs N rounds of processing (message-passing)
    steps. The input to the Core is the concatenation of the Encoder's output
    and the previous output of the Core (labeled "Hidden(t)" below, where "t" is
    the processing step).
  - A "Decoder" graph net, which independently decodes the edge, node, and
    global attributes (does not compute relations etc.), on each message-passing
    step.

                      Hidden(t)   Hidden(t+1)
                         |            ^
            *---------*  |  *------*  |  *---------*
            |         |  |  |      |  |  |         |
  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)
            |         |---->|      |     |         |
            *---------*     *------*     *---------*
  """

  def __init__(self,
               edge_output_size=None,
               node_output_size=None,
               global_output_size=None,
               name="EncodeProcessDecode"):
    super(EncodeProcessDecode, self).__init__(name=name)
    self._encoder = MLPGraphIndependent()
    self._core = MLPGraphNetwork()
    self._decoder = MLPGraphIndependent()
    # Transforms the outputs into the appropriate shapes.
    if edge_output_size is None:
      edge_fn = None
    else:
      edge_fn = lambda: SigmoidEdgeProcessingModule(edge_output_size)
    if node_output_size is None:
      node_fn = None
    else:
      node_fn = lambda: snt.Linear(node_output_size, name="node_output")
    if global_output_size is None:
      global_fn = None
    else:
      global_fn = lambda: snt.Linear(global_output_size, name="global_output")
    with self._enter_variable_scope():
      self._output_transform = modules.GraphIndependent(edge_fn, node_fn,
                                                        global_fn)

  def _build(self, input_op, num_processing_steps):
    latent = self._encoder(input_op)
    output_ops = []
    for _ in range(num_processing_steps):
      core_input = latent
      latent = self._core(core_input)
      decoded_op = self._decoder(latent)
      output_ops.append(self._output_transform(decoded_op))
    return output_ops


class GraphProcess(snt.AbstractModule):
  """Full encode-process-decode model.

  The model we explore includes three components:
  - An "Encoder" graph net, which independently encodes the edge, node, and
    global attributes (does not compute relations etc.).
  - A "Core" graph net, which performs N rounds of processing (message-passing)
    steps. The input to the Core is the concatenation of the Encoder's output
    and the previous output of the Core (labeled "Hidden(t)" below, where "t" is
    the processing step).
  - A "Decoder" graph net, which independently decodes the edge, node, and
    global attributes (does not compute relations etc.), on each message-passing
    step.
                t=t+1
      Hidden(t)<-----Hidden(t+1)
        |              ^
        |  *------*    |  *---------*
        |  |      |    |  |         |
        *->| Core |----*->| Decoder |---> Output(t)
           |      |       |         |
           *------*       *---------*
  """

  def __init__(self,
               dimension,
               atom_num,
               action_set_size,
               edge_output_size=None,
               node_output_size=None,
               global_output_size=None,
               name="GraphProcess"):
    super(GraphProcess, self).__init__(name=name)
    self._encoder = Encoder()
    self._core = MLPGraphNetwork()
    self._decoder = MLPGraphIndependent()
    self._edge_size = edge_output_size
    self._action_store_node = []
    self._action_store_edge = []
    self._dimension = dimension
    self._atom_num = atom_num
    self._action_model_fn = make_mlp_model()
    self._action_edges_model_fn = make_mlp_model()
    self._action_set_size = action_set_size
    self.p_node_store=[]
    self.p_node_model = make_mlp_model()
    self.output_action = []
    # Transforms the outputs into the appropriate shapes.
    if edge_output_size is None:
      edge_fn = None
    else:
      atom_state_edge_fn = lambda: SigmoidEdgeProcessingModule(edge_output_size)
    if node_output_size is None:
      node_fn = None
    else:
      node_fn = lambda: snt.Linear(node_output_size, name="node_output")
    if global_output_size is None:
      global_fn = None
    else:
      global_fn = lambda: snt.Linear(global_output_size, name="global_output")
    with self._enter_variable_scope():
      self._output_transform = modules.GraphIndependent(edge_model_fn=atom_state_edge_fn,
                                                          node_model_fn=node_fn, global_model_fn=global_fn)
    for index in range(0, self._action_set_size):
      edges = []
      nodes = (np.random.uniform(-6 / np.sqrt(self._dimension), 6 / np.sqrt(self._dimension), self._dimension))
      nodes = np.array(nodes, dtype='float32')
      nodes = tf.convert_to_tensor(nodes)
      nodes = tf.stack([nodes])
      updated_action_nodes = self._action_model_fn(nodes)
      self._action_store_node.append(updated_action_nodes)
      for vindex in range(1, self._atom_num + 1):
        temp = [np.random.uniform(-1, 1)]
        edges.append(temp)
      edges = tf.convert_to_tensor(edges)
      update_action_edges = [self._action_edges_model_fn(edges)]
      self._action_store_edge.append(update_action_edges)
    self._action_store_node = tf.concat(self._action_store_node, axis=0)
    self.output_action = self._action_store_node
    self._action_store_edge = tf.concat(self._action_store_edge, axis=0)

    nodes = (np.random.uniform(-6 / np.sqrt(self._dimension), 6 / np.sqrt(self._dimension), self._dimension*self._atom_num))
    nodes = np.reshape(nodes, (atom_num, dimension))
    nodes = np.array(nodes, dtype='float32')
    nodes = tf.convert_to_tensor(nodes)
    nodes = tf.stack(nodes)
    self.p_node_store = self.p_node_model(nodes)




  def action_replace(self, graph, action_index):
    updated_action_nodes = tf.gather(self._action_store_node, action_index)
    return graph.replace(globals=updated_action_nodes)

  def atomNode_replace(self, input_g):
    state_node = input_g.nodes[0]
    state_node = tf.stack([state_node])
    nodes = []
    nodes.append(state_node)
    nodes.append(self.p_node_store)
    nodes = tf.concat(nodes, axis=0)
    return input_g.replace(nodes=nodes)

  def stateNodeAdjustment(self, input_g):
    state_node = self.p_node_store[1]
    state_node = tf.stack([state_node])
    nodes = []
    nodes.append(state_node)
    nodes.append(self.p_node_store[1:])
    nodes = tf.concat(nodes, axis=0)
    return input_g.replace(nodes=nodes)

  def updateAction(self, graph, action_index, decoded):
    action_feature = graph.globals
    act_decoded = decoded.globals
    tmp = tf.constant(1)
    tmp2 = self._action_set_size - 1 - action_index
    split0_n, split1_n, split2_n = tf.split(self._action_store_node, [action_index, tmp, tmp2], 0)
    update_n = []
    update_n.append(split0_n)
    update_n.append(action_feature)
    update_n.append(split2_n)
    self._action_store_node = tf.concat(update_n, axis=0)

    tmp = tf.constant(1)
    tmp2 = self._action_set_size - 1 - action_index
    split0_n, split1_n, split2_n = tf.split(self.output_action, [action_index, tmp, tmp2], 0)
    update_n = []
    update_n.append(split0_n)
    update_n.append(act_decoded)
    update_n.append(split2_n)
    self.output_action = tf.concat(update_n, axis=0)

    self.p_node_store = graph.nodes[1:]


  def _build(self, input_op, action_sequence,num_processing_steps, flags):
    output_trans = input_op
    output_ops = []
    action_set = []
    all_vsg = []
    output_vecs = []
    output_ops.append(output_trans)
    for i in range(num_processing_steps):
        output_trans, _ = self.getStateVec(output_trans)
        output_vecs.append(output_trans)
        output_trans, latent = self.processOneStep(output_trans, action_sequence[i])
        output_ops.append(output_trans)
        action_set.append(output_trans.globals)
        if flags:
          self.updateAction(latent, action_sequence[i], output_trans)
    output_trans, _ = self.getStateVec(output_trans)
    output_vecs.append(output_trans)
    for out in output_vecs:
      all_vsg.append(out)
    return output_ops, action_set, all_vsg


  def getInnerRepresentation(self, graph):
    latent = self._encoder(graph)
    latent = self._core.getInitial(latent)
    latent = self._decoder(latent)
    return latent
  def getStateVec(self, graph):
    latent = self._encoder(graph)
    latent = self.stateNodeAdjustment(latent)
    action_index = [0]
    latent = self.action_replace(latent, action_index)
    latent = self.atomNode_replace(latent)
    latent = self._core.getInitial(latent)
    one = latent
    decoded_op = self._decoder(latent)
    output_trans = self._output_transform(decoded_op)
    return output_trans, one
  def getActionStore(self):
    return self.output_action


  def processOneStep(self, latent, action_index):
    latent = self._encoder(latent)

    core_input = self.action_replace(latent, [action_index])
    core_input = self.atomNode_replace(core_input)
    core_input = self._core.getInitial(core_input)
    latent = self._core(core_input)

    decoded_op = self._decoder(latent)
    output_trans = self._output_transform(decoded_op)
    return output_trans, latent


class HeuristicNetwork(snt.AbstractModule):
  def __init__(self, name="HeuristicNetwork", size=None):
    super(HeuristicNetwork, self).__init__(name=name)
    with self._enter_variable_scope():
      self._network = make_heu_mlp_model()
      if size is None:
        self.heu_fn = None
      else:
        self.heu_fn = snt.Linear(size, name="heu_output")

  def _build(self, inputs):
    n_out = self._network(inputs)
    return tf.nn.sigmoid(self.heu_fn(n_out))